{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96af8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:48:21.528638Z",
     "start_time": "2023-05-24T16:48:21.523913Z"
    }
   },
   "source": [
    "# Assignment 3 - Transformer Implemention with Andrej Karpathy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748aba8",
   "metadata": {},
   "source": [
    "In this notebook, I followed the code structure presented in the video. The first section consists of the breakdown of the transformer fundamentals, while the second section contains the fully implemented code. Throughout the video, the instructor explained the fundamentals and made adjustments to the fully coded notebook until it was completed. \n",
    "\n",
    "I will now provide a breakdown of my understanding of each part and address all the questions raised in the instructions. It's important to note that I did not directly copy the notebook from the instructor's repository. Instead, I used it as a reference to double-check the correctness of my code and made necessary corrections at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6955a",
   "metadata": {},
   "source": [
    "### What is a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f4783",
   "metadata": {},
   "source": [
    "A `language model` is a statistical model that captures the patterns and dependencies in a given language. It can generate the probabilities of the next sequence of characters or words based on the entire sequence. This is achieved by training the model on a large corpus of text, where it learns to predict the next word given the previous words in a sentence or sequence.\n",
    "\n",
    "Language models can perform various tasks in NLP, such as `Machine Translation`, `Sentiment Analysis`, `Text Completion`, and `Dialogue Generation`. This notebook focuses on implementing these tasks. One of the most famous language model families is the `GPT (Generative Pre-trained Transformer)` series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a91a1d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b3e2aa",
   "metadata": {},
   "source": [
    "The dataset used in this project is a subset of the original works from `William Shakespeare`. It includes the scripts from various plays and works by Shakespeare, which serve as the training data for our model. This dataset holds significance as it is one of Karpathy's favorite datasets and is commonly employed in NLP modeling tasks. Specific details about the dataset, including its preprocessing steps, can be found in the pre-processing section of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bb81c",
   "metadata": {},
   "source": [
    "### Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9c2f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T15:07:45.196588Z",
     "start_time": "2023-05-26T15:07:45.192140Z"
    }
   },
   "source": [
    "1. A sorted list of unique characters is stored in a variable.\n",
    "2. The size of the vocabulary is determined to set the embedding size.\n",
    "3. Two dictionaries, `stoi` (string to integers) and `itos` (integer to string), are created to map characters to integers and vice versa.\n",
    "4. The `encode` and `decode` functions utilize these dictionaries. The `encode` function converts a string into a list of corresponding integers using `stoi`, while the `decode` function converts a list of integers back into a string.\n",
    "5. The dataset is split into train and test sets based on the length of the encoded data. The training set contains 90% of the data, while the test set contains 10%.\n",
    "\n",
    "Overall, these steps involve creating mappings between characters and integers, providing functions for encoding and decoding, and performing the train-test split on the encoded data. These preparations are crucial for further data processing and modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e195d0",
   "metadata": {},
   "source": [
    "### What is self-attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372aa615",
   "metadata": {},
   "source": [
    "Self-attention is a technique commonly used in transformer-based architectures to capture relationships and dependencies within the `input sequence itself`. It involves associating each position in the sequence with three vectors: query, key, and value.\n",
    "\n",
    "- Query: Represents the focus word for which the context is being determined. The query vector is compared against other words in the sentence to measure relevance or similarity.\n",
    "- Key: Creates key vectors for all words in the sentence. These vectors help assess the relationship between the focus word (query vector) and other words. Higher similarity indicates a stronger relationship.\n",
    "- Value: Generates value vectors for all words, containing contextual information. The similarity scores between the query and key vectors are used to compute a weighted sum of the value vectors. The resulting weighted sum represents the attended representation, highlighting the elements that are most relevant for the final representation.\n",
    "\n",
    "In summary, self-attention allows each position in the sequence to focus on other positions `within the same sequence` **(intra-attention)**, using query, key, and value vectors. By measuring similarity and computing weighted sums, it captures contextual relationships and produces a comprehensive representation of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186931ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T16:10:55.078170Z",
     "start_time": "2023-05-26T16:10:55.070667Z"
    }
   },
   "source": [
    "### Compare and Contrast Attention, Self-Attention and Cross-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdf0f2",
   "metadata": {},
   "source": [
    "Attention:\n",
    "- `Attention` is a mechanism that allows a model to selectively focus on specific parts of the input sequence when generating the output.\n",
    "- This mechanism calculates the relevance or importance of different elements in the input sequence to a particular context or query.\n",
    "- In Homework 2, we learned about attention and its implementation, which involves linearly transforming the embedding and hidden state to obtain weights. These weights are then applied to the encoder outputs to derive the values. The values are subsequently concatenated back to the embedding and undergo another linear transformation to gather more information and focus the attention on higher-weighted elements.\n",
    "\n",
    "Self-Attention: \n",
    "- `Self-attention` is a variant of attention that focuses only within the same sequence, also known as intra-attention.\n",
    "- The code in Karpathy's video demonstrates a decoder self-attention, as it generates text based on what it has learned, without any translation involved.\n",
    "- For further explanation, please refer to the previous section.\n",
    "\n",
    "Cross-Attention:\n",
    "- `Cross-attention` is another variant of attention and an extension of self-attention, allowing the model to attend to different input sequences or modalities simultaneously.\n",
    "- Unlike self-attention, which operates within a single sequence, cross-attention enables interactions between multiple sequences.\n",
    "- It also uses Query, Key, and Value vectors. The Query vectors come from one sequence, representing the elements of interest, while the Key-Value vectors come from another sequence and provide contextual information.\n",
    "- Information Flow: In the transformer architecture, information from the encoder is directly passed to the multi-head attention mechanism of the decoder. This enables the decoder to attend to different parts of the encoded input sequence while generating the output sequence.\n",
    "\n",
    "Purpose:\n",
    "- `Attention` focuses on specific parts of the input sequence to generate an output, calculating relevance or importance.\n",
    "- `Self-attention` captures dependencies within a single sequence by measuring the relationship between elements.\n",
    "- `Cross-attention` aligns and exchanges information between different sequences to capture dependencies and align relevant information.\n",
    "\n",
    "Calculations:\n",
    "- `Attention` calculates the relevance or importance of different elements in the input sequence to a specific context or query.\n",
    "- `Self-attention` computes similarity scores between elements in the sequence using Query, Key, and Value vectors associated with each element.\n",
    "- `Cross-attention` computes similarity scores between elements in the Query sequence and Key vectors derived from the Key-Value sequence.\n",
    "\n",
    "Input and Output:\n",
    "- `Attention` takes an input sequence and generates an output based on the weighted relevance of different elements.\n",
    "- `Self-attention` operates within a single sequence, with each element associated with Query, Key, and Value vectors derived from the same sequence.\n",
    "- `Cross-attention` involves two sequences - a Query sequence and a Key-Value sequence - and computes relevance between elements in the Query sequence and elements in the Key-Value sequence.\n",
    "\n",
    "Relationship:\n",
    "- `Attention` considers the relevance or importance of elements within a single sequence.\n",
    "- `Self-attention` captures dependencies and patterns within a single sequence by measuring the relationship between elements.\n",
    "- `Cross-attention` aligns and exchanges information between different sequences, capturing dependencies and aligning relevant information from one sequence to another.\n",
    "\n",
    "In summary, attention, self-attention, and cross-attention serve different purposes and operate at different levels of granularity. Attention focuses on relevance within a sequence, self-attention captures dependencies within a single sequence, and cross-attention aligns and exchanges information between different sequences. They differ in terms of their calculations, input and output, and the relationships they capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074e282d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T16:46:04.255435Z",
     "start_time": "2023-05-26T16:46:04.248898Z"
    }
   },
   "source": [
    "### What is multi-head attention? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271da960",
   "metadata": {},
   "source": [
    "`Multi-head attention` refers to the concept of having multiple individual attention mechanisms, known as `heads`, operating independently and learning to focus on different parts of the input sequence. It can be visualized as separate attention modules that attend to specific aspects or perspectives of the input.\n",
    "\n",
    "In the context of transformers, `multihead attention` is a technique that extends the basic attention mechanism by employing multiple heads simultaneously. Each head performs its own attention calculations and learns different representations of the input. The outputs from the individual heads are then concatenated or combined to produce a final representation.\n",
    "\n",
    "The primary purpose of using multihead attention is to enable the model to capture diverse types of information and dependencies in the input sequence. By employing multiple heads, the model can attend to different aspects or viewpoints of the input concurrently, enhancing its capability to extract meaningful information and capture complex relationships. It provides a mechanism for the model to leverage various attention patterns and learn more expressive representations.\n",
    "\n",
    "In the code provided by Karpathy, `nn.ModuleList` is used to stack instances of the `Head` class, representing the individual attention heads. These heads are calculated in parallel, allowing for efficient computation. Additionally, linear transformations and dropout are applied to the outputs for normalization and regularization purposes, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1ade5",
   "metadata": {},
   "source": [
    "### What is a transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66f629",
   "metadata": {},
   "source": [
    "The **transformer architecture** is based on the concept of **self-attention mechanism**, which allows the model to weigh different parts of the input sequence differently based on their relevance to the current context. This process the input sequence in parallel and **capture dependencies between all elements** in the sequence using self-attention. Unlike traditional **recurrent neural networks (RNNs)** or **convolutional neural networks (CNNs)**, transformers do not rely on sequential processing or fixed-size convolutional windows.\n",
    "\n",
    "The key components of a transformer architecture include:\n",
    "\n",
    "1. The transformer architecture consists of an **encoder and a decoder**. The encoder processes the input sequence and generates hidden representations using **self-attention** and **feed-forward neural network** sub-layers. The decoder takes the encoder outputs and generates the output sequence using **self-attention** and **cross-attention** sub-layers.\n",
    "\n",
    "2. **Self-attention** is a key component of the transformer architecture. It allows the model to weigh different elements of the input sequence based on their relevance to each other. This is done by comparing the similarity between **query, key, and value vectors** associated with each input element.\n",
    "\n",
    "3. **Positional encoding** is used to provide information about the order or position of the elements in the input sequence. It helps the model understand the sequential information without relying on recurrent connections.\n",
    "\n",
    "Overall, the transformer architecture enables **parallel processing** of the input sequence, **captures dependencies between all elements** using self-attention, and incorporates **positional encoding** to handle sequential information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac7bf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:20:38.867291Z",
     "start_time": "2023-05-26T17:20:38.860819Z"
    }
   },
   "source": [
    "### What is the purpose of residual connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5933b17",
   "metadata": {},
   "source": [
    "Residual connections are a key technique in deep neural networks, which **transformer** is part of, to mitigate the **vanishing gradient problem** and facilitate gradient flow during training. By adding a **shortcut connection** that preserves the original input alongside the transformed output of a layer, the network can effectively propagate gradients and learn incremental changes. Karpathy clearly explained in the video why the bypass happens in the architecture.\n",
    "\n",
    "This mechanism allows the model to **bypass problematic layers** and learn an identity mapping when necessary, promoting smoother optimization and preserving important information. Overall, **residual connections** enable deep networks, including transformers, to effectively capture complex patterns and dependencies, leading to improved performance and training of deeper architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6432b20b",
   "metadata": {},
   "source": [
    "### What is the purpose of layer normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa5c23",
   "metadata": {},
   "source": [
    "Layer normalization is a technique used in transformers to **normalize the hidden representations** at each layer of the model. It ensures that the values have **zero mean and unit variance**, stabilizing the training process and improving the model's ability to capture complex patterns and dependencies in the data. \n",
    "\n",
    "Layer normalization is **preferred over other normalization techniques** in transformers due to its ability to handle **varying sequence lengths**. Unlike batch normalization, which computes statistics across a batch, layer normalization operates independently on each example in the sequence. This makes it suitable for tasks with variable-length sequences, such as natural language processing, where the length of sentences can vary significantly. Additionally, layer normalization has been shown to be more effective in handling small batch sizes, which is common in transformer models. It also helps mitigate the impact of the \"covariate shift\" problem and provides better generalization performance. \n",
    "\n",
    "Karpathy also **broken down the code** of the Layer Normalization applied in the nn module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e597a",
   "metadata": {},
   "source": [
    "### What is the purpose of dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27911f",
   "metadata": {},
   "source": [
    "Similar to what we've discussed in the CNN lecture, **Dropout** is a **regularization technique** used in transformers to prevent overfitting and improve generalization. It randomly sets a fraction of the input units to zero during training, **forcing the model to learn redundant representations** and reducing the reliance on specific features. In the context of transformers, dropout is typically applied to the output of each sub-layer in the encoder and decoder. \n",
    "\n",
    "By randomly dropping out units, dropout helps to **prevent the model from relying too heavily on individual elements** and encourages the network to learn more robust and generalizable representations. This regularization technique can improve the performance of transformers by reducing overfitting and improving their ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63df82",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f03f0",
   "metadata": {},
   "source": [
    "In this section I followed the pre-processing steps and coded along each of the sections he was explanaining in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80bab9",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "525ce317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:29.924714Z",
     "start_time": "2023-05-27T13:32:29.371795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-27 13:32:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.4’\n",
      "\n",
      "input.txt.4         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-05-27 13:32:29 (101 MB/s) - ‘input.txt.4’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetching dataset from Karpathy's repo\n",
    "\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41c4e446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:29.936284Z",
     "start_time": "2023-05-27T13:32:29.928480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "# Loading the content of the file and counting total characters\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f: \n",
    "    text = f.read()\n",
    "print(f'len of dataset in characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "70e39baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:29.942562Z",
     "start_time": "2023-05-27T13:32:29.938797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing first 1000 lines of the dataset\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "94f1f3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:29.998041Z",
     "start_time": "2023-05-27T13:32:29.972124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Get all unique characters sort it and its total len\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a354ff0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:30.809180Z",
     "start_time": "2023-05-27T13:32:30.798992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43, 2]\n",
      "hi there!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Initialization of String-to-Integer and Integer-to-String Dictionaries: \n",
    "    Initializing dictionaries to map each character to its corresponding index \n",
    "    and each index to its corresponding character. \n",
    "\n",
    "    Definition of encode and decode functions: \n",
    "    Defining functions to convert the text and list of integers into their \n",
    "    respective indexed values.\n",
    "\"\"\"\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "iots = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([iots[i] for i in l])\n",
    "\n",
    "print(encode(\"hi there!\"))\n",
    "print(decode(encode(\"hi there!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2716130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:31.287948Z",
     "start_time": "2023-05-27T13:32:31.033867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "#Encoding of the whole dataset and conversion to tensor object\n",
    "\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af8d5923",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:31.297218Z",
     "start_time": "2023-05-27T13:32:31.291862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into train (90%) and test (10%) splits.\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffa4ce",
   "metadata": {},
   "source": [
    "### Discussions on key code blocks of the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a614fdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:32.015822Z",
     "start_time": "2023-05-27T13:32:32.006438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The block size is the number of tokens in the sequence in this context. \n",
    "    It includes an additional +1 token since we aim to predict the next token\n",
    "    (character) in the sequence.\n",
    "\"\"\"\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "492c4d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:32.277126Z",
     "start_time": "2023-05-27T13:32:32.266695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Karpathy demonstrated and explained how the prediction works, showing that \n",
    "    the target (y) for every sequence within the entire block size is the next \n",
    "    character +1.\n",
    "\"\"\"\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "edf41902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:32.779821Z",
     "start_time": "2023-05-27T13:32:32.761979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the taget: 43\n",
      "when input is [24, 43] the taget: 58\n",
      "when input is [24, 43, 58] the taget: 5\n",
      "when input is [24, 43, 58, 5] the taget: 57\n",
      "when input is [24, 43, 58, 5, 57] the taget: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the taget: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the taget: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the taget: 39\n",
      "when input is [44] the taget: 53\n",
      "when input is [44, 53] the taget: 56\n",
      "when input is [44, 53, 56] the taget: 1\n",
      "when input is [44, 53, 56, 1] the taget: 58\n",
      "when input is [44, 53, 56, 1, 58] the taget: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the taget: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the taget: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the taget: 1\n",
      "when input is [52] the taget: 58\n",
      "when input is [52, 58] the taget: 1\n",
      "when input is [52, 58, 1] the taget: 58\n",
      "when input is [52, 58, 1, 58] the taget: 46\n",
      "when input is [52, 58, 1, 58, 46] the taget: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the taget: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the taget: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the taget: 46\n",
      "when input is [25] the taget: 17\n",
      "when input is [25, 17] the taget: 27\n",
      "when input is [25, 17, 27] the taget: 10\n",
      "when input is [25, 17, 27, 10] the taget: 0\n",
      "when input is [25, 17, 27, 10, 0] the taget: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the taget: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the taget: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the taget: 39\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Karpathy demonstrated and explained how the dataset is prepared for \n",
    "    training explained here that each sequence of data has maximimum of 8 \n",
    "    tokens as the block_size is 8 and 4 rows for each batch. This is shown\n",
    "    as the architecture can process matrices in parallel independent of each\n",
    "    other as we've also discussed in class.\n",
    "\"\"\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): #batch dimension\n",
    "    for t in range(block_size): #time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the taget: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e3bd51a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:33.487146Z",
     "start_time": "2023-05-27T13:32:33.479656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cbecdd64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:34:47.302930Z",
     "start_time": "2023-05-27T13:34:47.201538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "65\n",
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    This is the initial version of the BigramLanguageModel, where Karpathy \n",
    "    explained the shapes of B (batch_size), T (sequence length), and C \n",
    "    (channels/vocab_size). We discussed how logits-softmax works, which is the \n",
    "    same explanation we had in class. We also calculated the loss using \n",
    "    cross_entropy when the target is defined, as it measures the dissimilarity \n",
    "    between the predicted probability distribution over classes (our vocabulary) \n",
    "    and the true distribution (target token).\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the BigramLanguageModel.\n",
    "        \n",
    "        Args:\n",
    "            idx (torch.Tensor): Input tensor of shape (B, T) representing the indices of tokens.\n",
    "            targets (torch.Tensor): Target tensor of shape (B, T) representing the indices of target tokens.\n",
    "        \n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits tensor of shape (B, T, C) representing the predicted scores for each token.\n",
    "            loss (torch.Tensor or None): Loss tensor if targets are provided, else None.\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens based on the input sequence.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Input tensor of shape (B, T) representing the indices of tokens.\n",
    "            max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            idx (torch.Tensor): Generated tensor of shape (B, T+max_new_tokens) representing the updated sequence.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "print(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# Initiate idx with a 1,1 tensor to kick_off the generation of 100 new tokens\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "106cdfb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T13:32:33.932429Z",
     "start_time": "2023-05-27T13:32:33.926722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Same with Prof Basti's preferred optimizer AdamW per Karpathy it just works well\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41cd5ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:48:28.591951Z",
     "start_time": "2023-05-26T17:47:05.924597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.464580535888672\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Training loop demonstration demonstrating that the model is learning\n",
    "    with the loss being reduced.\n",
    "\"\"\"\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6af1197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:01.936411Z",
     "start_time": "2023-05-26T17:50:01.809735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHell dlenjut, t hin t 's ve het\n",
      "I f bes.\n",
      "\n",
      "MBar, wiathoffopooue pe best t e thtoufounive.\n",
      "LOFaishe thy s rigaty, geanuk-\n",
      "Whandestharyo wissad; aicouit, ply!\n",
      "Hew k s teawithiwind amatos--wowofepan cor nd CY heng Whonor as wingEx?\n",
      "MENENUCUKBo monmy ckichethtiXectoury'ind e s,\n",
      "Myo\n",
      "\n",
      "Ageayothe d faloury t oram. co dathintha vessorike silofeigar tongat mu in mow norra the ow athe aketifest; t w brothenete ousald tle ppl bere ovr?-t,\n",
      "\n",
      "Thelle if\n",
      "Whe! bu t tour bly: w ithin INTingaspounst d p hin, st h b\n"
     ]
    }
   ],
   "source": [
    "# Quality is still pretty bad but better than the initial output\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc432ef9",
   "metadata": {},
   "source": [
    "### This section demonstrated how masking works for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "608a3452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:02.579174Z",
     "start_time": "2023-05-26T17:50:02.570450Z"
    }
   },
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "# Calculating the mean of the previous tokens for each position in the sequence\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a761ba16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:03.381781Z",
     "start_time": "2023-05-26T17:50:03.374590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2 # batch, time channels\n",
    "x = torch.randn(B, T, C) # generating a random tensor as inputs\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19d6dd96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:03.963747Z",
     "start_time": "2023-05-26T17:50:03.954014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]]) tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Demonstrated tril function how it returns the lower triangle and one of the\n",
    "    most important aspects in transformers on how matrix multiplication can speed up\n",
    "    all the calculations and having the same behavior as the previous loop \n",
    "    in calculating the mean by comparing xbow and xbow2 ouput which is True.\n",
    "\"\"\"\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x #(B, T, T) @ (B, T, C) -----> (B, T, C)\n",
    "print(torch.allclose(xbow, xbow2))\n",
    "print(xbow[0], xbow2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "239794ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:04.747201Z",
     "start_time": "2023-05-26T17:50:04.735250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]]) tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    In this code block, Karpathy discussed why we set -inf for all the 0 values \n",
    "    in the upper triangle portion. This is the masking process, which aims to\n",
    "    prevent attention scores from being assigned to those positions and exclude \n",
    "    them from subsequent softmax normalization.\n",
    "\"\"\"\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "\n",
    "print(torch.allclose(xbow, xbow3))\n",
    "print(xbow[0], xbow2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4619f790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:06.552560Z",
     "start_time": "2023-05-26T17:50:06.534978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32])\n",
      "torch.Size([4, 8, 16])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   This is the most important discussion in the video where he discussed how \n",
    "   self-attention works and explained step-by-step the key, query, and value \n",
    "   matrices, as well as why they are linearly transformed.\n",
    "\n",
    "   He dissected the calculation of query and key multiplication, explaining \n",
    "   how    it finds the most relevant values within a given sequence and how it \n",
    "   prevents information from the future using masking. After applying softmax \n",
    "   normalization, the resulting attention weights are used to multiply with \n",
    "   the value matrix, obtaining the contextual information.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # Defining the values for batch size, time dimension, and channels\n",
    "x = torch.randn(B, T, C)\n",
    "print(x.shape)\n",
    "\n",
    "# Head for self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "# Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# out = wei @ x\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "print(out.shape)\n",
    "print(wei[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4d7cb3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:07.241581Z",
     "start_time": "2023-05-26T17:50:07.236518Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The code demonstrates scaling the head size by squaring it to control the\n",
    "    magnitude of self-attention weights. This prevents them from becoming too \n",
    "    large or too small, stabilizing the learning process and addressing issues \n",
    "    like vanishing and exploding gradients. The scaling factor is applied \n",
    "    during the computation of query and key values in self-attention.\n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92dbe617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:07.570256Z",
     "start_time": "2023-05-26T17:50:07.564027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26bebcd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:07.817255Z",
     "start_time": "2023-05-26T17:50:07.807298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc0693b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:08.142822Z",
     "start_time": "2023-05-26T17:50:08.132246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b827d1e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:08.428995Z",
     "start_time": "2023-05-26T17:50:08.418606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "376605d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:09.076254Z",
     "start_time": "2023-05-26T17:50:09.037601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1335, -0.1059, -0.3824,  ..., -1.3422, -0.1971,  0.8795],\n",
       "        [-0.0353, -0.7439, -0.3371,  ..., -0.6276, -0.4846,  0.4556],\n",
       "        [ 0.3069, -1.5010,  1.4898,  ..., -0.6819,  0.9993,  0.8382],\n",
       "        ...,\n",
       "        [-1.6080, -1.6324, -0.7634,  ..., -0.9847,  0.0039, -0.8610],\n",
       "        [-0.2273,  0.0066, -0.2763,  ..., -0.8705, -1.2442, -0.7531],\n",
       "        [ 0.3054, -0.1505, -0.3809,  ..., -1.4962, -0.7711, -1.0681]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Karpathy shared the code block to demonstrate how the LayerNorm code works. \n",
    "    He simplified the original computation to this version and explained why \n",
    "    it is preferred over batch normalization, as discussed in the earlier \n",
    "    section of this notebook.\n",
    "\"\"\"\n",
    "\n",
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "    \"\"\"\n",
    "    Custom implementation of Layer Normalization for 1D input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of LayerNorm1d.\n",
    "        \"\"\"\n",
    "        xmean = x.mean(1, keepdim=True)  # Calculate batch mean\n",
    "        xvar = x.var(1, keepdim=True)  # Calculate batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the parameters of the LayerNorm1d module.\n",
    "        \"\"\"\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100)  # Batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "print(x.shape)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112c684",
   "metadata": {},
   "source": [
    "## Full Model From Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "829100e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:50:56.712436Z",
     "start_time": "2023-05-26T17:50:56.652229Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Created it as a function to easily change parameters\n",
    "def nano_GPT (batch_size=16, block_size=32, max_iters=5000, eval_interval=100,\n",
    "              learning_rate=1e-3, eval_iters=200, n_embd=64, n_head=4,\n",
    "              n_layer=4, dropout=0.0):\n",
    "    \n",
    "#     batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "#     block_size = 32 # what is the maximum context length for predictions?\n",
    "#     max_iters = 5000\n",
    "#     eval_interval = 100\n",
    "#     learning_rate = 1e-3\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     eval_iters = 200\n",
    "#     n_embd = 64\n",
    "#     n_head = 4\n",
    "#     n_layer = 4\n",
    "#     dropout = 0.0\n",
    "#     # ------------\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "\n",
    "    # wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # here are all the unique characters that occur in this text\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    # create a mapping from characters to integers\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    encode = lambda s: [stoi[c] for c in s] \n",
    "    decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "    # Train and test splits\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    # data loading\n",
    "    def get_batch(split):\n",
    "        # generate a small batch of data of inputs x and targets y\n",
    "        data = train_data if split == 'train' else val_data\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss():\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "    class Head(nn.Module):\n",
    "        \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "        def __init__(self, head_size):\n",
    "            super().__init__()\n",
    "            self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            B,T,C = x.shape\n",
    "            k = self.key(x)   # (B,T,C)\n",
    "            q = self.query(x) # (B,T,C)\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "            wei = self.dropout(wei)\n",
    "            # perform the weighted aggregation of the values\n",
    "            v = self.value(x) # (B,T,C)\n",
    "            out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "            return out\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "        def __init__(self, num_heads, head_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "            self.proj = nn.Linear(n_embd, n_embd)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out\n",
    "\n",
    "    class FeedFoward(nn.Module):\n",
    "\n",
    "        def __init__(self, n_embd):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4 * n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * n_embd, n_embd),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "        def __init__(self, n_embd, n_head):\n",
    "            super().__init__()\n",
    "            head_size = n_embd // n_head\n",
    "            self.sa = MultiHeadAttention(n_head, head_size)\n",
    "            self.ffwd = FeedFoward(n_embd)\n",
    "            self.ln1 = nn.LayerNorm(n_embd)\n",
    "            self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "            x = x + self.ffwd(self.ln2(x))\n",
    "            return x\n",
    "\n",
    "    # super simple bigram model\n",
    "    class BigramLanguageModel(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "            self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "            self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "            self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "            self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        def forward(self, idx, targets=None):\n",
    "            B, T = idx.shape\n",
    "            tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "            x = tok_emb + pos_emb # (B,T,C)\n",
    "            x = self.blocks(x) # (B,T,C)\n",
    "            x = self.ln_f(x) # (B,T,C)\n",
    "            logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "            if targets is None:\n",
    "                loss = None\n",
    "            else:\n",
    "                B, T, C = logits.shape\n",
    "                logits = logits.view(B*T, C)\n",
    "                targets = targets.view(B*T)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            return logits, loss\n",
    "\n",
    "        def generate(self, idx, max_new_tokens):\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                # crop idx to the last block_size tokens\n",
    "                idx_cond = idx[:, -block_size:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_cond)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx\n",
    "\n",
    "    model = BigramLanguageModel()\n",
    "    m = model.to(device)\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # generate from the model\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f191c2",
   "metadata": {},
   "source": [
    "### Fine Tuning the Model with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3a7b2",
   "metadata": {},
   "source": [
    "Executed with just the base parameters from the repository and will be used as the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62000ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:56:58.629862Z",
     "start_time": "2023-05-26T17:50:57.201985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4198, val loss 2.4337\n",
      "step 400: train loss 2.3499, val loss 2.3561\n",
      "step 500: train loss 2.2963, val loss 2.3127\n",
      "step 600: train loss 2.2408, val loss 2.2499\n",
      "step 700: train loss 2.2057, val loss 2.2191\n",
      "step 800: train loss 2.1636, val loss 2.1869\n",
      "step 900: train loss 2.1241, val loss 2.1507\n",
      "step 1000: train loss 2.1025, val loss 2.1294\n",
      "step 1100: train loss 2.0696, val loss 2.1187\n",
      "step 1200: train loss 2.0376, val loss 2.0789\n",
      "step 1300: train loss 2.0242, val loss 2.0641\n",
      "step 1400: train loss 1.9917, val loss 2.0361\n",
      "step 1500: train loss 1.9703, val loss 2.0313\n",
      "step 1600: train loss 1.9626, val loss 2.0489\n",
      "step 1700: train loss 1.9414, val loss 2.0140\n",
      "step 1800: train loss 1.9078, val loss 1.9946\n",
      "step 1900: train loss 1.9081, val loss 1.9885\n",
      "step 2000: train loss 1.8840, val loss 1.9970\n",
      "step 2100: train loss 1.8713, val loss 1.9759\n",
      "step 2200: train loss 1.8609, val loss 1.9634\n",
      "step 2300: train loss 1.8559, val loss 1.9525\n",
      "step 2400: train loss 1.8406, val loss 1.9427\n",
      "step 2500: train loss 1.8169, val loss 1.9436\n",
      "step 2600: train loss 1.8236, val loss 1.9388\n",
      "step 2700: train loss 1.8143, val loss 1.9343\n",
      "step 2800: train loss 1.8024, val loss 1.9204\n",
      "step 2900: train loss 1.8055, val loss 1.9330\n",
      "step 3000: train loss 1.7952, val loss 1.9214\n",
      "step 3100: train loss 1.7714, val loss 1.9174\n",
      "step 3200: train loss 1.7530, val loss 1.9111\n",
      "step 3300: train loss 1.7569, val loss 1.9033\n",
      "step 3400: train loss 1.7539, val loss 1.8968\n",
      "step 3500: train loss 1.7354, val loss 1.8931\n",
      "step 3600: train loss 1.7298, val loss 1.8935\n",
      "step 3700: train loss 1.7273, val loss 1.8823\n",
      "step 3800: train loss 1.7195, val loss 1.8927\n",
      "step 3900: train loss 1.7212, val loss 1.8756\n",
      "step 4000: train loss 1.7111, val loss 1.8605\n",
      "step 4100: train loss 1.7118, val loss 1.8744\n",
      "step 4200: train loss 1.7086, val loss 1.8676\n",
      "step 4300: train loss 1.7025, val loss 1.8463\n",
      "step 4400: train loss 1.7051, val loss 1.8635\n",
      "step 4500: train loss 1.6880, val loss 1.8466\n",
      "step 4600: train loss 1.6871, val loss 1.8313\n",
      "step 4700: train loss 1.6829, val loss 1.8418\n",
      "step 4800: train loss 1.6657, val loss 1.8385\n",
      "step 4900: train loss 1.6710, val loss 1.8401\n",
      "step 4999: train loss 1.6666, val loss 1.8198\n",
      "\n",
      "And they bride will that were madie;\n",
      "Thou but tarth that weal tauge art that us hath but redilaccao,\n",
      "Away, my fearst, where me\n",
      "Yout proof it.\n",
      "But this nowle, and if ensent, will is the overtage.\n",
      "\n",
      "WARGENV:\n",
      "All just, lest dise of thus queen,\n",
      "Now up have the tyban's nou nor\n",
      "To this lost my liked me, and on in on him evicks to the\n",
      "Murth high, is he poor of his but\n",
      "pray nobrurt for treagint me.\n",
      "\n",
      "PRIARE:\n",
      "Uncta, affarry, I hom.\n",
      "\n",
      "HENRY BOLINGBROY:\n",
      "\n",
      "Shose Warwick, stavoin courtear tear repts I\n",
      "am the while me: ank, I sum horse of bream of a cempres-spend;\n",
      "But with reason; and being wilt weep on,\n",
      "Thou confessyy stratchs lome\n",
      "This sontly evers.\n",
      "\n",
      "LUCIO:\n",
      "Abere ove Lady Canscatom's countent boy\n",
      "Of the hasth a gRilates:y, contly gravent\n",
      "Whose is deed, is xughion the bear\n",
      "that fraving with some. Do self For these lack.\n",
      "\n",
      "PRAMININGHAS IS:\n",
      "No?\n",
      "Their his divinger, do this mister,\n",
      "Houstimed the now the summon approad'd whose no crow out.\n",
      "\n",
      "ESBRALANA:\n",
      "Now, thy getry is.\n",
      "\n",
      "BENSINCE:\n",
      "You the glear Gly thee, our her and brune.\n",
      "\n",
      "POMPEY:\n",
      "Own no; go whill conve, do windlisand, and to you bring!\n",
      "\n",
      "AMINIUS:\n",
      "Gitters to tears his in infect high him a wordn,\n",
      "And left braned his foIndsmy a canies to may incerance\n",
      "In to dot me attele true exre brotengity.\n",
      "\n",
      "DUKE VINGING\n",
      "BRADTUS:\n",
      "Most'st He whertuse me;\n",
      "For there'er the endseman the vown;\n",
      "Mad there to but but his forhward\n",
      "But lothy doth'dl they sich the beguts\n",
      "Good vonburt Rithmons, the baelings whom the deenums ome,\n",
      "Sirral you there plains. Me our son one old,\n",
      "Then them signnest what them wars, evis;\n",
      "River thou a--a scace:\n",
      "Out.\n",
      "\n",
      "PORALNIA:\n",
      "By nurge, ristrake, you all prevons year try.\n",
      "\n",
      "BENVOLIO:\n",
      "Now.\n",
      "From I may wound arm that sure give,\n",
      "Befock, mah, sinstats o' be, God the have blay not,\n",
      "That bring begire man I heav toge of the hamt,\n",
      "Threobly tall weak the sweett aftears' of trust:\n",
      "Ner honerous 'lift he putilt\n",
      "With is a scause; whom now.\n",
      "\n",
      "CORIOLPHEY:\n",
      "May, many and, and, Jom. It\n",
      "Take of to, Jury Roman!\n",
      "Nor brien shown's fortunded such to firther,\n",
      "For an wh\n"
     ]
    }
   ],
   "source": [
    "nano_GPT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717607f",
   "metadata": {},
   "source": [
    "In this iteration, the goal was to increase the number of iterations from `5000` to `10000` and reduce the learning rate from `1e-3` to `4e-3`. And we can see the improvement in the performance reducing the loss by around `.1` which is a good reduction at the expense of longer run time `6 min vs 16 min`\n",
    "\n",
    "As explained by Karpathy in the video, increasing the number of iterations helps the model learn better by allowing the parameters to be updated and optimizing performance over multiple training steps. This concept was also discussed in class, where longer training times for neural network models enable the optimization process to find its minima.\n",
    "\n",
    "A lower learning rate contributes to smoother convergence, improved generalization, finer parameter updates, and increased robustness to noisy data. By taking smaller steps during optimization, the model can make more stable and consistent progress, avoid overfitting, capture subtle patterns, and mitigate the impact of noisy samples.\n",
    "\n",
    "However, it is important to be cautious about potential overfitting, where the model becomes too specialized to the training data and performs poorly on unseen data, as we learned in class. Finding the optimal balance between convergence speed and generalization performance is crucial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f4d30a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T00:55:55.776647Z",
     "start_time": "2023-05-27T00:39:13.877289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4107, val loss 4.4039\n",
      "step 100: train loss 2.5200, val loss 2.5261\n",
      "step 200: train loss 2.3499, val loss 2.3556\n",
      "step 300: train loss 2.2482, val loss 2.2696\n",
      "step 400: train loss 2.1646, val loss 2.1872\n",
      "step 500: train loss 2.1079, val loss 2.1475\n",
      "step 600: train loss 2.0617, val loss 2.1174\n",
      "step 700: train loss 2.0114, val loss 2.0767\n",
      "step 800: train loss 1.9776, val loss 2.0631\n",
      "step 900: train loss 1.9415, val loss 2.0413\n",
      "step 1000: train loss 1.9047, val loss 2.0121\n",
      "step 1100: train loss 1.8918, val loss 2.0046\n",
      "step 1200: train loss 1.8679, val loss 1.9788\n",
      "step 1300: train loss 1.8586, val loss 1.9532\n",
      "step 1400: train loss 1.8373, val loss 1.9709\n",
      "step 1500: train loss 1.8196, val loss 1.9483\n",
      "step 1600: train loss 1.8034, val loss 1.9414\n",
      "step 1700: train loss 1.7848, val loss 1.9299\n",
      "step 1800: train loss 1.7660, val loss 1.9145\n",
      "step 1900: train loss 1.7662, val loss 1.9107\n",
      "step 2000: train loss 1.7529, val loss 1.9101\n",
      "step 2100: train loss 1.7382, val loss 1.9033\n",
      "step 2200: train loss 1.7311, val loss 1.8742\n",
      "step 2300: train loss 1.7217, val loss 1.8868\n",
      "step 2400: train loss 1.7233, val loss 1.8954\n",
      "step 2500: train loss 1.7170, val loss 1.8660\n",
      "step 2600: train loss 1.6997, val loss 1.8628\n",
      "step 2700: train loss 1.7001, val loss 1.8640\n",
      "step 2800: train loss 1.6941, val loss 1.8486\n",
      "step 2900: train loss 1.6756, val loss 1.8373\n",
      "step 3000: train loss 1.6728, val loss 1.8257\n",
      "step 3100: train loss 1.6732, val loss 1.8401\n",
      "step 3200: train loss 1.6724, val loss 1.8506\n",
      "step 3300: train loss 1.6558, val loss 1.8182\n",
      "step 3400: train loss 1.6658, val loss 1.8324\n",
      "step 3500: train loss 1.6467, val loss 1.8162\n",
      "step 3600: train loss 1.6439, val loss 1.7964\n",
      "step 3700: train loss 1.6432, val loss 1.8105\n",
      "step 3800: train loss 1.6421, val loss 1.8124\n",
      "step 3900: train loss 1.6415, val loss 1.8128\n",
      "step 4000: train loss 1.6317, val loss 1.8084\n",
      "step 4100: train loss 1.6269, val loss 1.7978\n",
      "step 4200: train loss 1.6308, val loss 1.8008\n",
      "step 4300: train loss 1.6338, val loss 1.7821\n",
      "step 4400: train loss 1.6182, val loss 1.7902\n",
      "step 4500: train loss 1.6274, val loss 1.7984\n",
      "step 4600: train loss 1.6239, val loss 1.7992\n",
      "step 4700: train loss 1.6189, val loss 1.7953\n",
      "step 4800: train loss 1.6086, val loss 1.7740\n",
      "step 4900: train loss 1.6038, val loss 1.7709\n",
      "step 5000: train loss 1.6048, val loss 1.7672\n",
      "step 5100: train loss 1.5976, val loss 1.7646\n",
      "step 5200: train loss 1.6043, val loss 1.7552\n",
      "step 5300: train loss 1.5917, val loss 1.7751\n",
      "step 5400: train loss 1.5928, val loss 1.7739\n",
      "step 5500: train loss 1.5938, val loss 1.7728\n",
      "step 5600: train loss 1.5974, val loss 1.7759\n",
      "step 5700: train loss 1.5802, val loss 1.7590\n",
      "step 5800: train loss 1.5927, val loss 1.7649\n",
      "step 5900: train loss 1.5889, val loss 1.7590\n",
      "step 6000: train loss 1.5892, val loss 1.7782\n",
      "step 6100: train loss 1.5739, val loss 1.7656\n",
      "step 6200: train loss 1.5765, val loss 1.7484\n",
      "step 6300: train loss 1.5825, val loss 1.7561\n",
      "step 6400: train loss 1.5727, val loss 1.7482\n",
      "step 6500: train loss 1.5603, val loss 1.7547\n",
      "step 6600: train loss 1.5638, val loss 1.7575\n",
      "step 6700: train loss 1.5708, val loss 1.7468\n",
      "step 6800: train loss 1.5583, val loss 1.7415\n",
      "step 6900: train loss 1.5621, val loss 1.7316\n",
      "step 7000: train loss 1.5607, val loss 1.7449\n",
      "step 7100: train loss 1.5649, val loss 1.7327\n",
      "step 7200: train loss 1.5600, val loss 1.7399\n",
      "step 7300: train loss 1.5585, val loss 1.7365\n",
      "step 7400: train loss 1.5575, val loss 1.7441\n",
      "step 7500: train loss 1.5510, val loss 1.7402\n",
      "step 7600: train loss 1.5469, val loss 1.7356\n",
      "step 7700: train loss 1.5463, val loss 1.7324\n",
      "step 7800: train loss 1.5484, val loss 1.7494\n",
      "step 7900: train loss 1.5447, val loss 1.7326\n",
      "step 8000: train loss 1.5418, val loss 1.7480\n",
      "step 8100: train loss 1.5493, val loss 1.7422\n",
      "step 8200: train loss 1.5559, val loss 1.7377\n",
      "step 8300: train loss 1.5478, val loss 1.7335\n",
      "step 8400: train loss 1.5469, val loss 1.7313\n",
      "step 8500: train loss 1.5442, val loss 1.7148\n",
      "step 8600: train loss 1.5456, val loss 1.7299\n",
      "step 8700: train loss 1.5481, val loss 1.7235\n",
      "step 8800: train loss 1.5340, val loss 1.7259\n",
      "step 8900: train loss 1.5307, val loss 1.7191\n",
      "step 9000: train loss 1.5293, val loss 1.7222\n",
      "step 9100: train loss 1.5415, val loss 1.7223\n",
      "step 9200: train loss 1.5317, val loss 1.7323\n",
      "step 9300: train loss 1.5287, val loss 1.7307\n",
      "step 9400: train loss 1.5445, val loss 1.7216\n",
      "step 9500: train loss 1.5391, val loss 1.7208\n",
      "step 9600: train loss 1.5315, val loss 1.7215\n",
      "step 9700: train loss 1.5339, val loss 1.7272\n",
      "step 9800: train loss 1.5301, val loss 1.7348\n",
      "step 9900: train loss 1.5220, val loss 1.7304\n",
      "step 9999: train loss 1.5237, val loss 1.7178\n",
      "\n",
      "And they bride.\n",
      "\n",
      "NORTOLYCUS:\n",
      "Kman!\n",
      "The Balbule.\n",
      "Short not day, and bart thy pustory to becteen!\n",
      "Have away, my face, and zuton,\n",
      "Yourselve time you take much but slife,\n",
      "Even in latest in overs, and the next is wander liking me littish, courtey's carraises, why.\n",
      "\n",
      "SIBPULINGBROKEN:\n",
      "To gindevell, demoth?\n",
      "\n",
      "God, not when evill: where dost will no deeds of the enfore\n",
      "To kind thrust first; if he must with all on,\n",
      "To fier From Sometale-bury! still is:\n",
      "ards be his great hoin courtey, tell rents I am not!\n",
      "\n",
      "JOHne to Poldial:\n",
      "And yet his eymes on the sea-sport;\n",
      "But with reace the cause of the welcome,\n",
      "Thou couldsty to the might. \n",
      "GLOUCESTEPMEEN:\n",
      "Sweshind Perbellooy, Lady Caster of:\n",
      "And meet my see: in double! 'girlains: there is gracing\n",
      "Whose loudse need out to danger.\n",
      "\n",
      "TYBALT:\n",
      "Being wear soul. Doven, no more so lack,\n",
      "That yet cry himself?\n",
      "Their hath brother, do thing see I do it,\n",
      "Be died not the subming protesters\n",
      "Be than the ruiles, were be thou said;\n",
      "For for me watch time wout in forc;\n",
      "To know our sertanning was. Their fellown in euch not\n",
      "And how spiritisator and toath, I had!\n",
      "\n",
      "AUTOLYCUS:\n",
      "Findeed; tell this in in-sovereign,\n",
      "Honour did child friend! and be none\n",
      "my a canience and finderanced keep,\n",
      "To me attell tructes in twenn'd my haw--each I did;\n",
      "Save thou for Henceframents that prison;\n",
      "All conscajes me friend, Maughter and but but the fair caperjust torment:'\n",
      "Fall you cher's beguage to give the Richmond\n",
      "lave boe; I not was he and unto me,\n",
      "Sirrae you there passaso, I am children up thee\n",
      "elding sing.\n",
      "\n",
      "HASTINGS:\n",
      "Be the geves; for but of a-father my\n",
      "sun.\n",
      "\n",
      "Preteem; upone Bolingbrail:\n",
      "I come, yet not shame, any parfort,\n",
      "The tongue may man wound and ligger.\n",
      "\n",
      "ISABELLA:\n",
      "I love him, by anside not toward.\n",
      "But yet yond enter, be flauting father\n",
      "That I complit, of monge, would talk with the sworn fair?\n",
      "But I hope bury you here, i'liff he puts for the dukes:\n",
      "And wilt not fanture, try prozens and awonse trongar attender flan,\n",
      "Till Ricance, to swifts men's foot, honour me; he had noble, and I\n"
     ]
    }
   ],
   "source": [
    "nano_GPT(batch_size=16, block_size=32, max_iters=10000, eval_interval=100,\n",
    "              learning_rate=4e-3, eval_iters=400, n_embd=64, n_head=4,\n",
    "              n_layer=4, dropout=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077048ce",
   "metadata": {},
   "source": [
    "In this scenario, I increased the `batch_size` (doubled) and `block_size` (multiplied by 4) to assess if there would be any improvements. Additionally, I added dropout to normalize the model and evaluate its impact on the loss. As anticipated, there was a noticeable improvement of approximately `0.08x` compared to the previous iteration, which is a significant reduction with minimal increase in run time `16 min vs 17 min` and minimal increase in parameters by `6k+`.\n",
    "\n",
    "Increasing the `block_size`, which represents the number of tokens in a sequence, allows the transformer to capture longer dependencies and contextual information. This is particularly advantageous for tasks that require understanding larger contexts, such as document-level language modeling or generating long-range sequences.\n",
    "\n",
    "Increasing the `batch_size`, which refers to the number of sequences processed in parallel during training, can result in more efficient computation and better utilization of hardware resources. It enables parallelization across multiple examples, leading to faster training and improved overall throughput. Moreover, larger batch sizes can provide a more stable gradient estimate, leading to more consistent updates and potentially better convergence.\n",
    "\n",
    "Adding dropout regularization helps prevent overfitting and can enhance the model's generalization capability. Dropout randomly sets a portion of the model's activations to zero during training, forcing the model to learn redundant representations and making it more robust to noise in the input data. This regularization technique prevents the model from relying too heavily on specific features or patterns in the training data, resulting in better performance on unseen data.\n",
    "\n",
    "It's important to note that the effectiveness of increasing `block_size`, `batch_size`, and adding dropout may vary depending on the specific task, dataset, and model architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da84841f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T03:55:41.490154Z",
     "start_time": "2023-05-27T03:37:43.850509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.215873 M parameters\n",
      "step 0: train loss 4.3519, val loss 4.3466\n",
      "step 200: train loss 2.3933, val loss 2.4105\n",
      "step 400: train loss 2.1842, val loss 2.2179\n",
      "step 600: train loss 2.0131, val loss 2.0737\n",
      "step 800: train loss 1.8813, val loss 1.9898\n",
      "step 1000: train loss 1.8016, val loss 1.9317\n",
      "step 1200: train loss 1.7451, val loss 1.8870\n",
      "step 1400: train loss 1.7105, val loss 1.8568\n",
      "step 1600: train loss 1.6827, val loss 1.8328\n",
      "step 1800: train loss 1.6562, val loss 1.8025\n",
      "step 2000: train loss 1.6406, val loss 1.7984\n",
      "step 2200: train loss 1.6214, val loss 1.7863\n",
      "step 2400: train loss 1.6038, val loss 1.7732\n",
      "step 2600: train loss 1.5942, val loss 1.7706\n",
      "step 2800: train loss 1.5819, val loss 1.7456\n",
      "step 3000: train loss 1.5742, val loss 1.7380\n",
      "step 3200: train loss 1.5607, val loss 1.7334\n",
      "step 3400: train loss 1.5616, val loss 1.7456\n",
      "step 3600: train loss 1.5532, val loss 1.7299\n",
      "step 3800: train loss 1.5414, val loss 1.7226\n",
      "step 4000: train loss 1.5347, val loss 1.7140\n",
      "step 4200: train loss 1.5285, val loss 1.7041\n",
      "step 4400: train loss 1.5230, val loss 1.6973\n",
      "step 4600: train loss 1.5177, val loss 1.6970\n",
      "step 4800: train loss 1.5116, val loss 1.6959\n",
      "step 5000: train loss 1.5114, val loss 1.6991\n",
      "step 5200: train loss 1.5047, val loss 1.6943\n",
      "step 5400: train loss 1.5014, val loss 1.6919\n",
      "step 5600: train loss 1.4991, val loss 1.6787\n",
      "step 5800: train loss 1.4933, val loss 1.6782\n",
      "step 6000: train loss 1.4891, val loss 1.6788\n",
      "step 6200: train loss 1.4878, val loss 1.6722\n",
      "step 6400: train loss 1.4800, val loss 1.6652\n",
      "step 6600: train loss 1.4815, val loss 1.6588\n",
      "step 6800: train loss 1.4772, val loss 1.6649\n",
      "step 7000: train loss 1.4776, val loss 1.6689\n",
      "step 7200: train loss 1.4755, val loss 1.6603\n",
      "step 7400: train loss 1.4711, val loss 1.6614\n",
      "step 7600: train loss 1.4648, val loss 1.6512\n",
      "step 7800: train loss 1.4609, val loss 1.6592\n",
      "step 8000: train loss 1.4653, val loss 1.6527\n",
      "step 8200: train loss 1.4571, val loss 1.6463\n",
      "step 8400: train loss 1.4576, val loss 1.6427\n",
      "step 8600: train loss 1.4613, val loss 1.6455\n",
      "step 8800: train loss 1.4567, val loss 1.6508\n",
      "step 9000: train loss 1.4484, val loss 1.6391\n",
      "step 9200: train loss 1.4503, val loss 1.6434\n",
      "step 9400: train loss 1.4520, val loss 1.6450\n",
      "step 9600: train loss 1.4437, val loss 1.6309\n",
      "step 9800: train loss 1.4469, val loss 1.6410\n",
      "step 9999: train loss 1.4473, val loss 1.6303\n",
      "\n",
      "I will seeveign by to good shall breat;\n",
      "To speel he plead-dish ance!\n",
      "\n",
      "CiTINGBELLA:\n",
      "Nor me dind me as side bell manner on her this\n",
      "brids the forth inbrort: swooke which clootature\n",
      "See with us to kings windon warr. Leave my sory\n",
      "Come to fall Exate the cride of sheak about-whidh down.\n",
      "Is, thou mine love, take the thender; in this more\n",
      "That hath a own wan ams from none toot to perous,\n",
      "When to spoke our bewame sword some weeps;\n",
      "Liked in else in Good Sucking peace.\n",
      "Blade York and me.\n",
      "Peep and ofrom them,\n",
      "And confame higholdire, ranto him; till thy\n",
      "Clouward not with our enlent.\n",
      "With sepurshmer, Hergeis bold from in chille:\n",
      "'Tis valied rement was. Poun my him,\n",
      "Pay with shoold with brail'd befile they mise all\n",
      "place of drighter than would stil him in home.\n",
      "\n",
      "CORIOLANUS:\n",
      "In dose us be him none bow: I corls thee.\n",
      "In the wear, art seake mercy her entend\n",
      "Grave Clarenby a brese one\n",
      "Now your soldow; for how proglow'd not mysear\n",
      "With hork of yours. Conscome.\n",
      "Thre is golded his friarther.\n",
      "\n",
      "FRIAR CARIOLAND:\n",
      "Seememble these anon, dray the spear mercers make\n",
      "The shord mige, genar me him communit\n",
      "A royal sentleme, reland that cast ere I do have.\n",
      "\n",
      "GREY Hery BONY:\n",
      "With yet, far me shardself, their riduses word one\n",
      "Will strend, I hour's doing a people of hearity?\n",
      "\n",
      "TRANIO:\n",
      "Turn's is home me; cold their to.\n",
      "\n",
      "LUCIONINUS:\n",
      "You came now'd us in awe Norfort fresold Well;\n",
      "You have your'd indeedity me,\n",
      "To he is, you to Auther cheedry,\n",
      "Vaust and kinder, you Edwarwick. Why I am talice!\n",
      "\n",
      "LNEP MOMERIO:\n",
      "Comes with my lif!\n",
      "\n",
      "Shepherd:\n",
      "I'll stand are Content.\n",
      "\n",
      "QUEEN ELIZAGALANANTIO:\n",
      "Came you will know now.\n",
      "3 KING. LooNG EDWAURE:\n",
      "There's yoer affeirs, is.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "No;\n",
      "This nighters g'et ears, forth's an histend soul,\n",
      "Which a mountrents brought of my underuratains,\n",
      "I dideath be willow. Haud like to his owens,\n",
      "At rebeles in hourdiands,\n",
      "Look its bold mercy infice bush\n",
      "and ourself accomplous, these infent, my and englanue\n",
      "Thoae churade such my ta'et tempert,\n",
      "Ait, being, forther: and merchd; and at me at,\n"
     ]
    }
   ],
   "source": [
    "nano_GPT(batch_size=32, block_size=128, max_iters=10000, eval_interval=200,\n",
    "              learning_rate=5e-3, eval_iters=400, n_embd=64, n_head=8,\n",
    "              n_layer=4, dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bb8ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this scenario, I increased the `multihead` to `16`, `block_size` to `256`, and `n_layers` to `8`. We can see here the biggest improvement at the expense of a significant increase in runtime.\n",
    "\n",
    "Increasing the number of layers and `multihead` attention in a transformer model provides several benefits. Firstly, it enhances the model's capacity to capture complex patterns and dependencies in the data. Each additional layer introduces more non-linear transformations, enabling the model to learn more intricate representations and make more sophisticated predictions. This increased capacity is particularly advantageous for tasks that involve intricate relationships and require the model to capture both local and global dependencies. The hierarchical feature extraction is improved as well, with lower layers capturing low-level features and higher layers capturing more abstract and contextual information. This hierarchical representation can be valuable for tasks that require a deep understanding of the input sequence.\n",
    "\n",
    "Secondly, the inclusion of `multihead` attention in the transformer architecture brings its own advantages. By utilizing `multiple attention heads`, the model can attend to different parts of the input sequence simultaneously. This allows for the capture of diverse and complementary information, leading to more effective attention mechanisms. `Multihead` attention enables the model to effectively capture long-range dependencies and attend to relevant parts of the input. This is particularly beneficial for tasks that involve understanding complex relationships and dependencies between tokens.\n",
    "\n",
    "Increasing the `number of layers` and `multihead` attention in the model introduces higher computational and memory demands, alongside the other adjusted parameters. Consequently, it is crucial to assess the impact on runtime and potentially quantify the trade-off between runtime and the achieved loss reduction. Additionally, qualitative evaluation of the model's output can provide insights into its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250ae82",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-27T13:37:45.612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.423233 M parameters\n",
      "step 0: train loss 4.3897, val loss 4.3945\n",
      "step 1000: train loss 1.6063, val loss 1.7758\n",
      "step 2000: train loss 1.4652, val loss 1.6617\n",
      "step 3000: train loss 1.4111, val loss 1.6239\n",
      "step 4000: train loss 1.3742, val loss 1.5842\n",
      "step 5000: train loss 1.3539, val loss 1.5819\n",
      "step 6000: train loss 1.3335, val loss 1.5608\n",
      "step 7000: train loss 1.3223, val loss 1.5601\n",
      "step 8000: train loss 1.3104, val loss 1.5467\n",
      "step 9000: train loss 1.3007, val loss 1.5358\n",
      "step 10000: train loss 1.2960, val loss 1.5401\n",
      "step 11000: train loss 1.2856, val loss 1.5273\n",
      "step 12000: train loss 1.2778, val loss 1.5225\n",
      "step 13000: train loss 1.2776, val loss 1.5268\n",
      "step 14000: train loss 1.2724, val loss 1.5272\n",
      "step 15000: train loss 1.2665, val loss 1.5112\n",
      "step 16000: train loss 1.2615, val loss 1.5077\n"
     ]
    }
   ],
   "source": [
    "nano_GPT(batch_size=64, block_size=256, max_iters=20000, eval_interval=1000,\n",
    "              learning_rate=5e-3, eval_iters=2000, n_embd=64, n_head=16,\n",
    "              n_layer=8, dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31724e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-27T16:49:35.557Z"
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa7f3a",
   "metadata": {},
   "source": [
    "To summarize my approach in fine-tuning, I focused on increasing the model's parameters while reducing the learning rate. By doing so, the model is able to learn more effectively and identify complex patterns within our dataset. This straightforward approach aimed to demonstrate potential improvements, but it is important to acknowledge the limitations of our search space and consider the need for additional datasets and further iterations to validate the findings.\n",
    "\n",
    "I chose not to increase the embedding layer since we are specifically detecting character sequences. Given the length and combination of characters within each line, the impact of increasing the embedding layer would likely be minimal.\n",
    "\n",
    "It is worth noting that even though we may achieve lower loss, the quality of the generated output may not match the original writing. This is because we are using the sequence of individual characters as tokens, rather than sequences of words. As a result, some of the words may appear obscure or incomprehensible. It is important to recognize that this is a decoder model, and if we were to input a different dataset unrelated to Shakespeare, its performance would likely be poor as it relies on the unique writing techniques found within the Shakespearean text.\n",
    "\n",
    "In terms of the video by Karpathy, his explanations were clear and insightful, providing a solid understanding of the fundamentals. From a coding perspective, I feel confident in building and utilizing this architecture and implementing it on my own. However, the challenge lies in finding an interesting and meaningful use case to apply it to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d26aa8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-27T15:06:23.827Z"
    }
   },
   "source": [
    "## Generative AI Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58b501",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-27T15:08:48.578Z"
    }
   },
   "source": [
    "This is how I used ChatGPT to assist me in the assignemnt:\n",
    "    \n",
    "- First I written off my understanding of each from Karpathy's video then asked ChatGPT to validate it and fixed the gramamr and point out any explanation mistakes. The context is in the usage within Neural Networks and Transformers.\n",
    "    - Language Model\n",
    "    - Attention\n",
    "    - Self-Attention\n",
    "    - Cross-Attention\n",
    "    - Multi-head Attention\n",
    "    - Transformer\n",
    "    - Residual Connections\n",
    "    - Layer Normalization\n",
    "    - Dropout\n",
    "- Assist in structuring the comparison and contrasts between the 3 attention.\n",
    "- Assist in interpreting the code blocks to align chat gpt with Karpathy's explanation, I did not find anything wrong with the responses.\n",
    "- Most of the time now I asked to give both layman and technical explanation from a data scientist point of view to help me explain it easier to someone else.\n",
    "- Helped me with adding comments in the code.\n",
    "- Helped me fix all the sentences grammars and spell checks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b466bac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-27T15:13:18.214Z"
    }
   },
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3248bc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-27T15:59:16.062Z"
    }
   },
   "source": [
    "1. Vaswani, A. (2017, June 12). Attention Is All You Need. arXiv.org. https://arxiv.org/abs/1706.03762\n",
    "2. Analytics Vidhya. (2023). A Comprehensive Guide to Attention Mechanism in Deep Learning for Everyone. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/\n",
    "3. What exactly are keys, queries, and values in attention mechanisms? (n.d.). Cross Validated. https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\n",
    "4. Ismali, F. (n.d.). GPT-4 explaining Self-Attention Mechanism. www.linkedin.com. https://www.linkedin.com/pulse/gpt-4-explaining-self-attention-mechanism-fatos-ismali/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f80be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
